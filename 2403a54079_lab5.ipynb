{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "6sOJCHBQtc4l",
        "outputId": "1e3e1329-5b25-4d50-bd30-46f45f06fe94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                titles  \\\n",
              "0    Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1    FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2    Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3    Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4    Background-Foreground Segmentation for Interio...   \n",
              "..                                                 ...   \n",
              "995  DeepIGeoS: A Deep Interactive Geodesic Framewo...   \n",
              "996  3D Densely Convolutional Networks for Volumetr...   \n",
              "997  UI-Net: Interactive Artificial Neural Networks...   \n",
              "998        One-Shot Learning for Semantic Segmentation   \n",
              "999  Exploring and Exploiting Diversity for Image S...   \n",
              "\n",
              "                                             summaries  \\\n",
              "0    Stereo matching is one of the widely used tech...   \n",
              "1    The recent advancements in artificial intellig...   \n",
              "2    In this paper, we proposed a novel mutual cons...   \n",
              "3    Consistency training has proven to be an advan...   \n",
              "4    To ensure safety in automated driving, the cor...   \n",
              "..                                                 ...   \n",
              "995  Accurate medical image segmentation is essenti...   \n",
              "996  In the isointense stage, the accurate volumetr...   \n",
              "997  For complex segmentation tasks, fully automati...   \n",
              "998  Low-shot learning methods for image classifica...   \n",
              "999  Semantic image segmentation is an important co...   \n",
              "\n",
              "                                                 terms  \n",
              "0                                   ['cs.CV', 'cs.LG']  \n",
              "1                          ['cs.CV', 'cs.AI', 'cs.LG']  \n",
              "2                                   ['cs.CV', 'cs.AI']  \n",
              "3                                            ['cs.CV']  \n",
              "4                                   ['cs.CV', 'cs.LG']  \n",
              "..                                                 ...  \n",
              "995                                          ['cs.CV']  \n",
              "996                                          ['cs.CV']  \n",
              "997  ['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE', '68T05, 6...  \n",
              "998                                          ['cs.CV']  \n",
              "999                                          ['cs.CV']  \n",
              "\n",
              "[1000 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b07ea72-71aa-45b6-aa4d-2fea415f3eda\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>DeepIGeoS: A Deep Interactive Geodesic Framewo...</td>\n",
              "      <td>Accurate medical image segmentation is essenti...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>3D Densely Convolutional Networks for Volumetr...</td>\n",
              "      <td>In the isointense stage, the accurate volumetr...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>UI-Net: Interactive Artificial Neural Networks...</td>\n",
              "      <td>For complex segmentation tasks, fully automati...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG', 'cs.NE', '68T05, 6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>One-Shot Learning for Semantic Segmentation</td>\n",
              "      <td>Low-shot learning methods for image classifica...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>Exploring and Exploiting Diversity for Image S...</td>\n",
              "      <td>Semantic image segmentation is an important co...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b07ea72-71aa-45b6-aa4d-2fea415f3eda')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6b07ea72-71aa-45b6-aa4d-2fea415f3eda button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6b07ea72-71aa-45b6-aa4d-2fea415f3eda');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module\",\n          \"Studying the Plasticity in Deep Convolutional Neural Networks using Random Pruning\",\n          \"A Gentle Introduction to Deep Learning in Medical Image Processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"LIDAR point clouds and RGB-images are both extremely essential for 3D object\\ndetection. So many state-of-the-art 3D detection algorithms dedicate in fusing\\nthese two types of data effectively. However, their fusion methods based on\\nBirds Eye View (BEV) or voxel format are not accurate. In this paper, we\\npropose a novel fusion approach named Point-based Attentive Cont-conv\\nFusion(PACF) module, which fuses multi-sensor features directly on 3D points.\\nExcept for continuous convolution, we additionally add a Point-Pooling and an\\nAttentive Aggregation to make the fused features more expressive. Moreover,\\nbased on the PACF module, we propose a 3D multi-sensor multi-task network\\ncalled Pointcloud-Image RCNN(PI-RCNN as brief), which handles the image\\nsegmentation and 3D object detection tasks. PI-RCNN employs a segmentation\\nsub-network to extract full-resolution semantic feature maps from images and\\nthen fuses the multi-sensor features via powerful PACF module. Beneficial from\\nthe effectiveness of the PACF module and the expressive semantic features from\\nthe segmentation module, PI-RCNN can improve much in 3D object detection. We\\ndemonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3D\\nDetection benchmark, and our method can achieve state-of-the-art on the metric\\nof 3D AP.\",\n          \"Recently there has been a lot of work on pruning filters from deep\\nconvolutional neural networks (CNNs) with the intention of reducing\\ncomputations.The key idea is to rank the filters based on a certain criterion\\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\\nfilters are pruned away the remainder of the network is fine tuned and is shown\\nto give performance comparable to the original unpruned network. In this work,\\nwe report experiments which suggest that the comparable performance of the\\npruned network is not due to the specific criterion chosen but due to the\\ninherent plasticity of deep neural networks which allows them to recover from\\nthe loss of pruned filters once the rest of the filters are fine-tuned.\\nSpecifically we show counter-intuitive results wherein by randomly pruning\\n25-50% filters from deep CNNs we are able to obtain the same performance as\\nobtained by using state-of-the-art pruning methods. We empirically validate our\\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\\nneeds to be tested on only a small set of classes at test time (say, only\\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\\nclass specific pruning and show that even here a random pruning strategy gives\\nclose to state-of-the-art performance. Unlike existing approaches which mainly\\nfocus on the task of image classification, in this work we also report results\\non object detection and image segmentation. We show that using a simple random\\npruning strategy we can achieve significant speed up in object detection (74%\\nimprovement in fps) while retaining the same accuracy as that of the original\\nFaster RCNN model. Similarly we show that the performance of a pruned\\nSegmentation Network (SegNet) is actually very similar to that of the original\\nunpruned SegNet.\",\n          \"This paper tries to give a gentle introduction to deep learning in medical\\nimage processing, proceeding from theoretical foundations to applications. We\\nfirst discuss general reasons for the popularity of deep learning, including\\nseveral major breakthroughs in computer science. Next, we start reviewing the\\nfundamental basics of the perceptron and neural networks, along with some\\nfundamental theory that is often omitted. Doing so allows us to understand the\\nreasons for the rise of deep learning in many application domains. Obviously\\nmedical image processing is one of these areas which has been largely affected\\nby this rapid progress, in particular in image detection and recognition, image\\nsegmentation, image registration, and computer-aided diagnosis. There are also\\nrecent trends in physical simulation, modelling, and reconstruction that have\\nled to astonishing results. Yet, some of these approaches neglect prior\\nknowledge and hence bear the risk of producing implausible results. These\\napparent weaknesses highlight current limitations of deep learning. However, we\\nalso briefly discuss promising approaches that might be able to resolve these\\nproblems in the future.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 133,\n        \"samples\": [\n          \"['cs.CV', 'eess.IV', 'stat.AP', '62P99']\",\n          \"['cs.CV', 'cs.LG', 'I.2.1, I.4.6,']\",\n          \"['cs.CV', 'cs.CR', 'cs.LG']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta2ygWHRj-bO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/arxiv_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset (first 1000 rows for safety)\n",
        "df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs (http, https, www)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove social media mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters (keep only alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace (reduce multiple spaces to single space)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to 'summaries' column\n",
        "df['processed_summaries'] = df['summaries'].apply(preprocess_text)\n",
        "\n",
        "# Preview results\n",
        "print(df[['summaries', 'processed_summaries']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAdt-Ah5lSjL",
        "outputId": "5f79c436-8263-4b19-fbde-a519177d0a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                                 processed_summaries  \n",
            "0  stereo matching is one of the widely used tech...  \n",
            "1  the recent advancements in artificial intellig...  \n",
            "2  in this paper we proposed a novel mutual consi...  \n",
            "3  consistency training has proven to be an advan...  \n",
            "4  to ensure safety in automated driving the corr...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['processed_summaries'] = df['summaries'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "r978js3jrHKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # required in newer NLTK versions\n",
        "\n",
        "df['tokenized_summaries'] = df['processed_summaries'].apply(lambda x: word_tokenize(x))\n",
        "print(df[['processed_summaries', 'tokenized_summaries']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftycYW7Nrfn6",
        "outputId": "8bc45b5d-9b2e-4c92-eebb-927479eed3af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 processed_summaries  \\\n",
            "0  stereo matching is one of the widely used tech...   \n",
            "1  the recent advancements in artificial intellig...   \n",
            "2  in this paper we proposed a novel mutual consi...   \n",
            "3  consistency training has proven to be an advan...   \n",
            "4  to ensure safety in automated driving the corr...   \n",
            "\n",
            "                                 tokenized_summaries  \n",
            "0  [stereo, matching, is, one, of, the, widely, u...  \n",
            "1  [the, recent, advancements, in, artificial, in...  \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...  \n",
            "3  [consistency, training, has, proven, to, be, a...  \n",
            "4  [to, ensure, safety, in, automated, driving, t...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure the stopwords resource is available\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define the set of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stopwords from tokenized_summaries\n",
        "df['filtered_summaries'] = df['tokenized_summaries'].apply(\n",
        "    lambda tokens: [w for w in tokens if w.lower() not in stop_words]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['tokenized_summaries', 'filtered_summaries']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF7Qm3x5opNS",
        "outputId": "18f16d8a-2494-403a-c8f3-77c7b16a0f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 tokenized_summaries  \\\n",
            "0  [stereo, matching, is, one, of, the, widely, u...   \n",
            "1  [the, recent, advancements, in, artificial, in...   \n",
            "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
            "3  [consistency, training, has, proven, to, be, a...   \n",
            "4  [to, ensure, safety, in, automated, driving, t...   \n",
            "\n",
            "                                  filtered_summaries  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancements, artificial, intelligenc...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure the WordNet corpus is available\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # optional, improves lemmatization coverage\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Apply lemmatization to each token list\n",
        "df['lemmatized_summaries'] = df['filtered_summaries'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(w) for w in tokens]\n",
        ")\n",
        "\n",
        "# Preview results\n",
        "print(df[['filtered_summaries', 'lemmatized_summaries']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqHN9fM2o5EB",
        "outputId": "6e711b57-c986-4dad-b470-6ddb39b23169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  filtered_summaries  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancements, artificial, intelligenc...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                lemmatized_summaries  \n",
            "0  [stereo, matching, one, widely, used, techniqu...  \n",
            "1  [recent, advancement, artificial, intelligence...  \n",
            "2  [paper, proposed, novel, mutual, consistency, ...  \n",
            "3  [consistency, training, proven, advanced, semi...  \n",
            "4  [ensure, safety, automated, driving, correct, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rejoin lemmatized words into a single string\n",
        "df['clean_summaries'] = df['lemmatized_summaries'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# Preview results\n",
        "print(df[['lemmatized_summaries', 'clean_summaries']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11RmFPIdpJdC",
        "outputId": "33a431b3-ec28-406e-f106-a9167a0597ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                lemmatized_summaries  \\\n",
            "0  [stereo, matching, one, widely, used, techniqu...   \n",
            "1  [recent, advancement, artificial, intelligence...   \n",
            "2  [paper, proposed, novel, mutual, consistency, ...   \n",
            "3  [consistency, training, proven, advanced, semi...   \n",
            "4  [ensure, safety, automated, driving, correct, ...   \n",
            "\n",
            "                                     clean_summaries  \n",
            "0  stereo matching one widely used technique infe...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper proposed novel mutual consistency networ...  \n",
            "3  consistency training proven advanced semisuper...  \n",
            "4  ensure safety automated driving correct percep...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure necessary NLTK resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')   # required in newer NLTK versions\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Step 1: Define preprocessing function (regex cleaning)\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Remove hashtags (#hashtag)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove emojis\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Remove special characters (keep alphanumeric + spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Step 2: Unified NLTK preprocessing pipeline\n",
        "def nltk_preprocessing_pipeline(text):\n",
        "    # Regex cleaning\n",
        "    cleaned = preprocess_text(text)\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(cleaned)\n",
        "\n",
        "    # Stopword removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
        "\n",
        "    # Rejoin words\n",
        "    final_text = ' '.join(lemmatized_tokens)\n",
        "\n",
        "    return final_text\n",
        "\n",
        "# Step 3: Apply pipeline to dataset\n",
        "df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "df['clean_summaries_pipeline'] = df['summaries'].apply(nltk_preprocessing_pipeline)\n",
        "\n",
        "# Preview results\n",
        "print(df[['summaries', 'clean_summaries_pipeline']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd-DBGkXpWqy",
        "outputId": "292139fe-aaa6-48cd-e1ee-6f57af070f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "\n",
            "                            clean_summaries_pipeline  \n",
            "0  stereo matching one widely used technique infe...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper proposed novel mutual consistency networ...  \n",
            "3  consistency training proven advanced semisuper...  \n",
            "4  ensure safety automated driving correct percep...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Ensure necessary NLTK resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')   # required in newer NLTK versions\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # required in newer NLTK versions\n",
        "\n",
        "# Load dataset (first 1000 rows for safety)\n",
        "# df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000) # Comment this line out to avoid overwriting df\n",
        "\n",
        "# Step 1: Tokenize summaries (if not already done)\n",
        "df['tokenized_summaries'] = df['summaries'].apply(lambda x: word_tokenize(str(x)))\n",
        "\n",
        "# Step 2: Apply POS tagging to all tokenized summaries\n",
        "df['pos_summaries'] = df['tokenized_summaries'].apply(lambda tokens: pos_tag(tokens))\n",
        "\n",
        "# Preview results\n",
        "print(df[['tokenized_summaries', 'pos_summaries']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_iYdDOHudy4",
        "outputId": "98c0df5f-9ca0-47f4-8e95-3bba5e93bf02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 tokenized_summaries  \\\n",
            "0  [Stereo, matching, is, one, of, the, widely, u...   \n",
            "1  [The, recent, advancements, in, artificial, in...   \n",
            "2  [In, this, paper, ,, we, proposed, a, novel, m...   \n",
            "3  [Consistency, training, has, proven, to, be, a...   \n",
            "4  [To, ensure, safety, in, automated, driving, ,...   \n",
            "\n",
            "                                       pos_summaries  \n",
            "0  [(Stereo, NNP), (matching, NN), (is, VBZ), (on...  \n",
            "1  [(The, DT), (recent, JJ), (advancements, NNS),...  \n",
            "2  [(In, IN), (this, DT), (paper, NN), (,, ,), (w...  \n",
            "3  [(Consistency, NN), (training, NN), (has, VBZ)...  \n",
            "4  [(To, TO), (ensure, VB), (safety, NN), (in, IN...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_summaries_pipeline'] = df['summaries'].apply(nltk_preprocessing_pipeline)\n"
      ],
      "metadata": {
        "id": "ObA27s1W0snl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(df['clean_summaries_pipeline'][0])\n",
        "nouns = []\n",
        "verbs = []\n",
        "for token in doc:\n",
        " if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "  nouns.append(token.text)\n",
        " elif token.pos_ == \"VERB\":\n",
        "  verbs.append(token.text)\n",
        "noun_freq = Counter(nouns)\n",
        "verb_freq = Counter(verbs)\n",
        "print(\"Noun Frequency:\", noun_freq)\n",
        "print(\"Verb Frequency:\", verb_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK_6AkhJzRWs",
        "outputId": "5d6b93aa-14a7-4d3b-f405-b4fa26672444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun Frequency: Counter({'stereo': 5, 'matching': 3, 'image': 2, 'speed': 2, 'application': 2, 'segmentation': 2, 'network': 2, 'term': 2, 'technique': 1, 'depth': 1, 'topic': 1, 'research': 1, 'find': 1, 'navigation': 1, '3d': 1, 'reconstruction': 1, 'field': 1, 'correspondence': 1, 'area': 1, 'challenge': 1, 'development': 1, 'cue': 1, 'result': 1, 'architecture': 1, 'leverage': 1, 'advantage': 1, 'paper': 1, 'aim': 1, 'comparison': 1, 'state': 1, 'art': 1, 'accuracy': 1, 'importance': 1, 'realtime': 1})\n",
            "Verb Frequency: Counter({'used': 2, 'matching': 1, 'inferring': 1, 'owing': 1, 'become': 1, 'driving': 1, 'finding': 1, 'nontextured': 1, 'shown': 1, 'improve': 1, 'proposed': 1, 'give': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import pos_tag, ne_chunk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# Reload dataset safely\n",
        "df = pd.read_csv('arxiv_data.csv', engine='python', nrows=1000)\n",
        "\n",
        "# Step 1: Tokenize\n",
        "df['tokenized_summaries'] = df['summaries'].apply(lambda x: word_tokenize(str(x)))\n",
        "\n",
        "# Step 2: POS tagging\n",
        "df['pos_summaries'] = df['tokenized_summaries'].apply(lambda tokens: pos_tag(tokens))\n",
        "\n",
        "# Step 3: NER\n",
        "def extract_entities(pos_tags):\n",
        "    chunked = ne_chunk(pos_tags)\n",
        "    entities = []\n",
        "    for subtree in chunked:\n",
        "        if hasattr(subtree, 'label'):\n",
        "            entity = \" \".join([token for token, pos in subtree.leaves()])\n",
        "            entities.append((entity, subtree.label()))\n",
        "    return entities\n",
        "\n",
        "# Apply to a small sample first to test\n",
        "df_sample = df.head(10).copy()\n",
        "df_sample['named_entities'] = df_sample['pos_summaries'].apply(extract_entities)\n",
        "\n",
        "print(\"Sample named entities:\")\n",
        "print(df_sample[['summaries', 'named_entities']])\n",
        "\n",
        "# Step 4: Frequency analysis (on sample for speed)\n",
        "all_entities = []\n",
        "for ents in df_sample['named_entities']:\n",
        "    all_entities.extend([entity for entity, label in ents])\n",
        "\n",
        "entity_freq = Counter(all_entities)\n",
        "print(\"\\nTop 10 most frequent named entities in sample:\")\n",
        "print(entity_freq.most_common(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-2URZex0y0A",
        "outputId": "a3cf03a7-d3ff-4a2c-c214-734540ed6e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample named entities:\n",
            "                                           summaries  \\\n",
            "0  Stereo matching is one of the widely used tech...   \n",
            "1  The recent advancements in artificial intellig...   \n",
            "2  In this paper, we proposed a novel mutual cons...   \n",
            "3  Consistency training has proven to be an advan...   \n",
            "4  To ensure safety in automated driving, the cor...   \n",
            "5  High-quality training data play a key role in ...   \n",
            "6  Semantic segmentation of fine-resolution urban...   \n",
            "7  To mitigate the radiologist's workload, comput...   \n",
            "8  Generalising deep models to new data from new ...   \n",
            "9  The success of deep learning methods in medica...   \n",
            "\n",
            "                                      named_entities  \n",
            "0                                    [(Stereo, GPE)]  \n",
            "1  [(AI, ORGANIZATION), (AI, ORGANIZATION), (Euro...  \n",
            "2                                                 []  \n",
            "3  [(Consistency, GSP), (Atrial Segmentation, ORG...  \n",
            "4  [(Gaussian Mixture Models, PERSON), (GMM, ORGA...  \n",
            "5                         [(EdgeFlow, ORGANIZATION)]  \n",
            "6  [(Semantic, GPE), (CNNs, ORGANIZATION), (Visio...  \n",
            "7                      [(AutoEncoder, ORGANIZATION)]  \n",
            "8                                  [(Hence, PERSON)]  \n",
            "9                                                 []  \n",
            "\n",
            "Top 10 most frequent named entities in sample:\n",
            "[('EHT', 4), ('Mask', 3), ('AI', 2), ('GMM', 2), ('Morphological Snakes', 2), ('CNNs', 2), ('Stereo', 1), ('European', 1), ('Health', 1), ('Robustness', 1)]\n"
          ]
        }
      ]
    }
  ]
}